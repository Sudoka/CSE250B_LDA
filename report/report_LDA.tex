\documentclass[10pt]{article}

% required packages
\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}

% better tables
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

% margins
\setlength{\headwidth}{6.40in}
\pagestyle{fancy}
\addtolength{\textwidth}{1in}
\addtolength{\textheight}{1in}
\addtolength{\evensidemargin}{0.5in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\topmargin}{-0.5in}


% page headers
\fancyhead{} 
\fancyhead[LO,LE]{CSE 250B}
\fancyhead[RO,RE]{Project 3}


\title{Topic Classification using Latent Dirichlet Allocation}

\author{Adrian Guthals (aguthals@cs.ucsd.edu),\\
David Larson (dplarson@ucsd.edu),\\
\\
CSE 250B: Project \#3 \\
University of California, San Diego \\
}


\begin{document}

\maketitle


%-----------------------------------------------------------------------------
% ABSTRACT
%-----------------------------------------------------------------------------
\begin{abstract}
Collapsed Gibbs Sampling is evaluated as training algorithm for Latent Dirichlet Allocation (LDA), an unsupervised learning model. LDA models are trained to classify text documents from two datasets: Classic400, a colelction of English documents from three research areas (library science, aeronautics, and medicine); and KOS, a collection of English blog posts from dailykos.com. For both datasets, the documents were classified into three topics and the LDA is evaluated based on the clustering of documents and lists of the most commonly occurred words for each topic. LDA of the Classic400 dataset showed distinct clustering of topics, and the list of most common words for each topic were cohesive (topic 1: system, research, scientific; topic 2: boundary, layer, wing; topic 3: patients, ventricular, fatty). Meanwhile, LDA of the KOS dataset show less distinct clustering and the list of most common words for each topic showed overlap, specifically, all topics contained words related to the 2008 US President campaign and election (topic 1: bush, kerry, war; topic 2: republican, race, house; topic 3: november, kerry, voting). The poor results of the KOS dataset indicate a lack of variety in the the studied dataset, which is a 400 document subset of the original 3430 document KOS dataset, rather than an issue with the LDA or Gibbs Sampling method. Overall, Gibbs Sampling proved to be a good learning algorithm for LDA models.
\end{abstract}



%-----------------------------------------------------------------------------
% INTRODUCTION
%-----------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

\subsection{Latent Dirichlet Allocation}
\label{sec:lda}

Latent Dirichlet Allocation (LDA) is an unsupervised learning model for determining topic classification of documents based on their vocabulary. It assumes that the generation of the documents was a probalistic process, and therefore, the parameters used in the generation can be learned based on the results. Specifically, LDA assumes thegeneration used the Dirichlet distribution, which is defined as a probability density function of all multinomial parameter vectors. Mathematically it is defined as:
\begin{equation}
    p(\gamma | \alpha) = \frac{1}{D(\alpha)} \prod_{s=1}^{m} \gamma_{s}^{\alpha_s - 1} \\
\end{equation}
where $\alpha$ is the parameter vector of length $m$ and $D$ a normalizing constant
\begin{equation}
    D(\alpha) = \int_{\gamma} \prod_{s=1}^m \gamma_s^{\alpha_s - 1}
\end{equation}

$D$ can also be defined explicitly as
\begin{equation}
    D(\alpha) = \frac{\prod_{s=1}^m \Gamma(\alpha_s)}{\Gamma (\sum_{s=1}^m \alpha_s )}
\end{equation}

The LDA generative process is defined in \cite{CSE250B} as:
\begin{enumerate}
    \item Given: Dirichlet distribution using $\alpha$ (parameter vector length $K$)
    \item Given: Dirichlet distribution using $\beta$ (parameter vector length $V$)
    \item for topic 1 to K:
        \begin{enumerate}
            \item draw multinomial distribution using $\phi_k$ based on $\beta$
        \end{enumerate}
    \item for document 1 to M:
        \begin{enumerate}
            \item draw multinomial $\theta$ based on $\alpha$
            \item for each word in document
                \begin{enumerate}
                    \item draw topic $z$ using $\theta$
                    \item draw word $w$ using $\phi_z$
                \end{enumerate}
        \end{enumerate}
\end{enumerate}


\subsection{Collapsed Gibbs Sampling}
\label{sec:gibbs}
Collapsed Gibbs Sampling is a learning method for indirectly determining $\theta$ for each document and $\phi_k$ for each topic by inferring the hidden value $z$ for each word in each document \cite{CSE250B}. Learning of $\theta$ and $\phi_k$ is based on:
\begin{equation}
    p(z_i = j | \bar{z}', \bar{w}) \propto \frac{q_{j w_i}' + \beta_{w_i}}{\sum_t q_{jt}' + \beta_t} \frac{n_{mj}' + \alpha_j}{\sum_k n_{mk}' + \alpha_k}
\end{equation}
where $\bar{w}$ is the sequence of words in the vocabulary.



%-----------------------------------------------------------------------------
% ALGORITHMS
%-----------------------------------------------------------------------------
\section{Design and Analysis of Algorithms (Adrian)}
\label{sec:algorithms}

We implemented the LDA generative process as outlined in \cite{CSE250B}.

Discuss how we're implementing LDA and Gibbs Sampling.



%-----------------------------------------------------------------------------
% EXPERIMENTS
%-----------------------------------------------------------------------------
\section{Design of Experiments}
\label{sec:experiments}

%
% DATASETS
%
\subsection{Datasets}
Two datasets were classified using LDA: Classic400, a collection of English documents from three research areas (aeronautics, medicine, and library science); and KOS, a collection of English blog posts from dailykos.com (see Table \ref{tab:datasets} for details on their sizes) \cite{Classic400, KOS_dataset}. While the Classic400 dataset provided the true labels of its source topics, the KOS dataset provided no such information. Also, to reduce run times, we elected to use a reduced version of the KOS dataset containing only the first 400 of the original 3430 documents.


\begin{table}[t]
    \centering
    \ra{1.2}
    \begin{tabular}{@{} l l l l l @{}}
        \toprule
        \bf{Dataset} & \bf{Documents} & \bf{Vocabulary} & $\alpha$ & $\beta$ \\
        \midrule
        Classic400 & 400 & 6205 & 0.01 & 0.1 \\
        KOS        & 400 & 6906 & 0.01 & 0.1\\
        \bottomrule
    \end{tabular}
    \caption{Composition of the two datasets used in this study and the chosen Gibbs hyperparameters $\alpha$ and $\beta$. Note that the KOS dataset used is a reduced version of the original KOS dataset, which contains 3430 documents.}
    \label{tab:datasets}
\end{table}


%
% HYPERPARAMETERS
%
\subsection{Hyperparameters}
For both datasets we apply LDA with three topics ($K = 3$). Since $\alpha$ is postively correlated to the number of topics present per document and we are only considering three topics, we chose values of $\alpha << 1$, which implies that every document contains only a few topics. Likewise, we chose values of $\beta << 1$, which implies that every word in every document corresponds to a small number of topics. Previous studies have looked into the effect of $\alpha$ and $\beta$ on the cluserting results for LDA models, some going so far as to suggest likely values based on the number of topics in a dataset (see \cite{Griffiths2004}). 


For this study we chose to use a grid search method to determine the optimal values of $\alpha$ and $\beta$, anticipating that both values will be much less than one. We trained the LDA model using Gibbs sampling for $\alpha$ and $\beta$ values of $10^2, 10^1, ..., 10^{-4}$, and evaluated the values based on the coherence of each topics' most commonly occurred words. From this grid search we chosen to set $\alpha = 0.01$ and $\beta = 0.1$ for both datasets (see Table \ref{tab:datasets}).


%
% CONVERGENCE
%
\subsection{Stopping Criteria of Gibbs Sampling}
For both datasets we chose the Gibbs Sampling stopping criteria as 100 epochs. We tried 50, 75, ..., 200 epochs as the stopping criteria, but found that the Euclidean distance between the $\phi$ vectors to not change signifcantly for more than 100 epochs.

%
% OVERFITTING
%
\subsection{Overfitting}
As with many models, LDA is susceptible to overfitting. One approach to determine overfitting of a LDA model is to calculate the perplexity of the model (see \cite{Heinrich}).


%
% RESULTS
%
\subsection{Results}

\subsubsection{Document Topic Clustering}
Figure \ref{fig:cluserting} shows the clustering of documents relative to each topic as projected onto a triangular simplex, where the each vertex indicates $\theta = 1$ for the denoted topic.


\begin{figure}[t]
    \centering
    \mbox{\subfigure[Classic400]{\includegraphics[width=0.5\textwidth]{classic400_thetas.pdf}} \quad
        \subfigure[KOS]{\includegraphics[width=0.5\textwidth]{kos_thetas.pdf} }}
    \caption{Clustering of documents based on their $\theta_1$, $\theta_2$ and $\theta_3$, projected on a triangular simplex. Documents in Classic400 are labeled based on their true labels (topic 1: white circles with black edges, topic 2: black solid squares, topic 3: gray solid triangles) while documents in KOS are have the same marker label as the true labels are unknown.}
    \label{fig:cluserting}
\end{figure}


\subsubsection{Most Common Words Per Topic}
If a LDA model is setup to classify more than 4 topics, then a simplex plot will not be a viable method for checking the clustering of the results. Instead, lists of the most commonly occurred words per topic can be analyzed to gauge the performance of the LDA. Table \ref{tab:most_common} shows the most commonly occurred words for each topic of the two datasets. The most common words from Classic400 for each topic are all related and correspond well to the true topic labels (topic 1: medicine, topic 2: aeronautics, and topic 3: library science). Meanwhile, the most common words for KOS for each topic seem to overlap and be derived from same general topic, namely politics, or more specifically, the 2008 US Presidential campaign between George Bush and John Kerry. As the source of KOS is a political blog site, it is not surprising that politics shows up prominetly as a topic. However, the lack of diversity between the most common words of each topic and therefore lack of unique topics is more likely the result of selecting only the first 400 documents from the originald dataset rather than an issue with our LDA model implementation.



\begin{table}[t]
	\centering
    \ra{1.2}
	\begin{tabular}{@{} l l l l c l l l @{}}
        \toprule
        & \multicolumn{3}{c}{Classic400} & \phantom{abc} & \multicolumn{3}{c}{KOS}\\
        \cmidrule{2-4} \cmidrule{6-8}
        \bf{Rank} & \bf{Topic 1} & \bf{Topic 2} & \bf{Topic 3} && \bf{Topic 1} & \bf{Topic 2} & \bf{Topic 3} \\
        \midrule
         1 & system     & boundary      & patients      && bush             & republican    & november \\
         2 & research   & layer         & ventricular   && kerry            & race          & kerry \\
         3 & scientific & wing          & fatty         && war              & house         & voting \\
         4 & retrieval  & mach          & cases         && iraq             & party         & vote \\
         5 & problems   & supersonic    & nickels       && people           & elections     & bush \\
         6 & language   & wings         & left          && president        & campaign      & polls \\
         7 & science    & ratio         & acids         && general          & democratic    & poll \\
         8 & methods    & velocity      & aortic        && voters           & state         & governor \\ 
         9 & systems    & effects       & blood         && administration   & democrats     & senate \\
        10 & subject    & shocks        & normal        && time             & senate        & republicans \\
        \bottomrule
	\end{tabular}
	\caption{Ten most commonly occured words for each topic classification for the classic400 and KOS datasets. Rank indicates how frequently a word appears in each topic, with 1 being the most occurred.}
	\label{tab:most_common}
\end{table}



%-----------------------------------------------------------------------------
% CONCLUSION
%-----------------------------------------------------------------------------
\section{Findings and Lessons Learned (Adrian)}
\label{sec:conclusion}

Thoughts on: LDA as a model, Gibbs Sampling as a training method, performance issues, results of the experiments


%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{sources}


\end{document}
