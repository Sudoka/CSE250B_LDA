\documentclass[10pt]{article}

% required packages
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}

% better tables
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

% margins
\setlength{\headwidth}{6.40in}
\pagestyle{fancy}
\addtolength{\textwidth}{1in}
\addtolength{\textheight}{1in}
\addtolength{\evensidemargin}{0.5in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\topmargin}{-0.5in}


% page headers
\fancyhead{} 
\fancyhead[LO,LE]{CSE 250B}
\fancyhead[RO,RE]{Project 3}


\title{Topic Classification using Latent Dirichlet Allocation}

\author{Adrian Guthals (aguthals@cs.ucsd.edu),\\
David Larson (dplarson@ucsd.edu),\\
\\
CSE 250B: Project \#3 \\
University of California, San Diego \\
}


\begin{document}

\maketitle


%-----------------------------------------------------------------------------
% ABSTRACT
%-----------------------------------------------------------------------------
\begin{abstract}
    LDA, Gibbs sampling, topic classification of documents, datasets used, results and their meaning, conclusions
\end{abstract}



%-----------------------------------------------------------------------------
% INTRODUCTION
%-----------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Introduce topic classification of documents. Then transition into formal definitions of LDA and Gibbs Sampling.

Elkan's lecture notes \cite{CSE250B}


\subsection{Latent Dirichlet Allocation}
\label{sec:lda}

Latent Dirichlet Allocation (LDA) is

\begin{equation}
    p(\gamma | \alpha) = \frac{1}{D(\alpha)} \prod_{s=1}^{m} \gamma_{s}^{\alpha_s - 1}
\end{equation}

\begin{equation}
    D(\alpha) = \int_{\gamma} \prod_{s=1}^m \gamma_s^{\alpha_s - 1}
\end{equation}

\begin{equation}
    D(\alpha) = \frac{\prod_{s=1}^m \Gamma(\alpha_s)}{\Gamma (\sum_{s=1}^m \alpha_s )}
\end{equation}



\subsection{Gibbs Sampling}
\label{sec:gibbs}

\begin{equation}
    p(z_i = j | \bar{z}', \bar{w}) \propto \frac{q_{j w_i}' + \beta_{w_i}}{\sum_t q_{jt}' + \beta_t} \frac{n_{mj}' + \alpha_j}{\sum_k n_{mk}' + \alpha_k}
\end{equation}



%-----------------------------------------------------------------------------
% ALGORITHMS
%-----------------------------------------------------------------------------
\section{Design and Analysis of Algorithms}
\label{sec:algorithms}

Discuss how we're implementing LDA and Gibbs Sampling.



%-----------------------------------------------------------------------------
% EXPERIMENTS
%-----------------------------------------------------------------------------
\section{Design of Experiments}
\label{sec:experiments}

%
% DATASETS
%
\subsection{Datasets}
Two datasets: classic400 and KOS blog posts (from UCI Machine Learning database \cite{KOS_dataset}). KOS dataset was reduced from 3430 documents to 400 documents (vocabulary unchanged from original 6906 words).

The KOS dataset is derivaed from dailykos.com blog entries and consists of 3430 documents, a vocabulary of 6906 words and 467714 words total words in all documents. For the experiment we only used data on the first 400 documents.


%
% HYPERPARAMETERS
%
\subsection{Hyperparameters}
$\alpha = 50 / K$ and $\beta=0.01$ where $K$ is the number of topics. For all experiments we set $K=3$ and therefore $\alpha = 16.67$. Suggested originally by Griffiths and Steyvers (2004). May want to try other values.

Recall: large $\alpha$ for many topics per document and large $\beta$ for many topics per word. We only use 3 topics so $\alpha$ will probably be small.


%
% CONVERGENCE
%
\subsection{Convergence of Gibbs}
When do we decide to stop Gibbs


%
% RESULTS
%
\subsection{Results}

\subsubsection{Clustering}
Clustering of documents into three topics (reference simplex plots).

\subsubsection{Most Common Words Per Topic}
Ten most common words (in tables).

\begin{table}
	\centering
    \ra{1.2}
	\begin{tabular}{@{} l l l l c l l l @{}}
        \toprule
        & \multicolumn{3}{c}{Classic400} & \phantom{abc} & \multicolumn{3}{c}{KOS}\\
        \cmidrule{2-4} \cmidrule{6-8}
        \bf{Rank} & \bf{Topic 1} & \bf{Topic 2} & \bf{Topic 3} && \bf{Topic 1} & \bf{Topic 2} & \bf{Topic 3} \\
        \midrule
         1 & patients     & boundary     & wing         && & & \\
         2 & case         & layer        & mach         && & & \\
         3 & ventricular  & velocity     & supersonic   && & & \\
         4 & system       & field        & effects      && & & \\
         5 & research     & solution     & ratio        && & & \\
         6 & scientific   & plate        & wings        && & & \\
         7 & fatty        & problem      & shock        && & & \\
         8 & nickel       & free         & numbers      && & & \\
         9 & acids        & heat         & jet          && & & \\
        10 & aortic       & cylinder     & lift         && & & \\
        \bottomrule
	\end{tabular}
	\caption{Ten most commonly occured words for each topic classification for the classic400 and KOS datasets. Rank corresponds to how frequently a word appears in each topic (1=most occurred).}
	\label{tab:common_classic400}
\end{table}



%-----------------------------------------------------------------------------
% CONCLUSION
%-----------------------------------------------------------------------------
\section{Findings and Lessons Learned}
\label{sec:conclusion}

Thoughts on: LDA as a model, Gibbs Sampling as a training method, performance issues, results of the experiments


%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{sources}


\end{document}
